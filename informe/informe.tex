\documentclass[titlepage,a4paper,oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{adjustbox}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{minted}
\usepackage{booktabs}
\usepackage[english,spanish,es-noindentfirst,es-nosectiondot,es-nolists,
es-noshorthands,es-lcroman,es-tabla]{babel}
\usepackage{lmodern}             % Use Latin Modern fonts
\usepackage[T1]{fontenc}         % Better output when a diacritic/accent is used
\usepackage[utf8]{inputenc}      % Allows to input accented characters
\usepackage{textcomp}            % Avoid conflicts with siunitx and microtype
\usepackage{microtype}           % Improves justification and typography
\usepackage[svgnames,table,xcdraw]{xcolor}    % Svgnames option loads navy (blue) colour
\usepackage[hidelinks,urlcolor=blue]{hyperref}
\hypersetup{colorlinks=true, allcolors=Navy, pdfstartview={XYZ null null 1}}
\newtheorem{lemma}{Lema}
\usepackage[width=14cm,left=3.5cm,marginparwidth=3cm,marginparsep=0.35cm,
height=21cm,top=1.7cm,headsep=1cm, headheight=1.6cm, footskip=1.2cm, bottom=2.4cm]{geometry}
\usepackage{csquotes}
\usepackage{mathrsfs}
\usepackage{biblatex}
\addbibresource{informe.bib}
\usepackage[pdf]{graphviz}

\begin{document}

\begin{titlepage}
\title{
	75.26 \-- Simulación \\
    \large Facultad de Ingeniería\\
	Universidad de Buenos Aires
}
\author{
	Mermet, Ignacio Javier\\
	\texttt{98153}
}
\date{Julio 2023}

\maketitle

\end{titlepage}

\tableofcontents

\newpage

\section{Introducción}
En el presente trabajo se apunta a explicar el paper \textit{Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations}\cite{lu18d} de \textit{Lu et. al}.  Se introduce la motivación de los autores, una breve reseña de la literatura para explicar el estado del arte en el momento de publicación, se explican los puntos principales del paper y mostramos los resultados obtenidos al intentar replicar los resultados obtenidos en el paper.

\subsection{Entrenamiento de redes neuronales muy profundas}
Las redes neuronales convolucionales empezaron a dominar el campo de vision por computadora luego de la introducción de AlexNet \cite{DBLP:journals/corr/Krizhevsky14} en 2012. Una gran ventaja de esta familia de modelos es que aprenden mapas de \textit{features} junto con un clasificador en un proceso punta a punta. De este modo, no es necesario que se definan a priori las \textit{features} relevantes de la imagen o del dominio bajo estudio.

Desde entonces, se ha acelerado el estudio de estas familias de modelos, con aplicaciones varias en imágenes, lenguaje natural, sonido, procesamiento de señales, sistemas de recomendaciones, entre otras. Distintos papers elaboran sobre la importancia de la profundidad de dichas redes para lograr mejores resultados. Es decir, la performance, bajo alguna medida de performance relevante al problema, aumenta al agregar capas de parámetros libres o bloques con más neuronas. Sin embargo, el entrenamiento de modelos cada vez más grandes ha expuesto varios problemas relacionados a la convergencia del entrenamiento.

Dos problemas comunes a la hora de entrenar redes neuronales muy profundas es la explosión de gradientes y el desvanecimiento de gradientes. El primero se refiere a que los valores de los gradientes a medida que se propagan por la red toman valores exponencialmente más grandes, hasta tomar el valor \texttt{NaN}. De este valor, no se pueden recuperar, dado que las operaciones de matmul no se encuentran definidas. El segundo se refiere al caso contrario, donde los gradientes se vuelven progresivamente más pequeños y tienden a cero, frenando por completo el entrenamiento. En ambos casos, la convergencia de la red se ve afectada. Ambos problemas se hacen más aparentes en redes neuronales muy profundas. Estos problemas han sido, a grandes rasgos, resueltos mediante el uso de distintos métodos de regularización entre capas así como mediante inicialización normalizada de los parámetros de la red.

Sin embargo, aún convergiendo, una consecuencia de aumentar la profundidad de las redes, es que se evidencia una degradación en el error de aprendizaje: la precisión de la red se satura y luego empeora al intentar agregar más capas. Sería natural pensar que se trata de un caso de sobreajuste u \textit{overfitting}, pero no es el caso.


\subsection{Redes residuales}
Dentro de la literatura que trata de resolver este problema, \cite{DBLP:journals/corr/HeZRS15} propone lo que denomina aprendizaje de residuos para resolver el problema de degradación. En esta sección hago un breve resumen de \S 1 del paper de ResNet a fin de proveer el contexto necesario para el resto de esta monografía.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ex_cnns.png}
\caption{Ejemplo de CNNs}
\label{ex_cnn}
\end{figure}

La segunda CNN en \ref{ex_cnn} posee una capa mas de conv-maxpooling que la primera. Asumamos que las demás capas están copiadas de la primer CNN de la figura, con sus mismos pesos. La capa extra podría aprender un mapeo de identidad y por tanto tener \textit{al menos} la misma precisión que la primer CNN de la figura. Esta solución por construcción de una red efectivamente más profunda indicaría que una red más profunda debería tener un error de entrenamiento más chico que una red más chica, pero esto se contradice con distintos resultados experimentales.

En lugar de esperar que algún conjunto de capas aprendan un mapeo deseado, se les hace aprender un mapeo residual: deseando aprender el mapeo $\mathscr{H}(x)$, dejan que ese conjunto no lineal de capas aprenda $\mathscr{F}(x) = \mathscr{H}(x) - x$, de modo de obtener la función original como $\mathscr{F}(x) + x$. Los autores conjeturan que es más fácil optimizar el mapeo residual que el mapeo original. Volviendo a nuestro argumento de construcción, si el mapeo identidad fuera óptimo, sería más facil llevar el residual a cero que aprender $\mathscr{H}(x)$ como un mapeo de identidad.

Dicha formulación puede ser expresada como conexiones entre capas no contiguas de una red neuronal \cite{srivastava2015highway}. ResNet utiliza mapeos identidad como conexiones de "atajo" entre capas (\ref{resnet_block}), dado que no introducen costo computacional extra ni agregan parámetros libres a la red. La salida de estas conexiones simplemente se suma a la salida de la capa. De este modo, se puede optimizar por backpropagation. En \ref{resnet_torch} se ilustra su implementación utilizando \texttt{pytorch}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{images/resnet_block.png}
\caption{Bloque de ResNet}
\label{resnet_block}
\end{figure}


\begin{figure}[H]
\begin{minted}[mathescape,
               linenos,
               numbersep=5pt,
               gobble=0,
               frame=lines,
               framesep=2mm,
	       firstnumber=1]{python}
class ResidualBlock(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int | None = None,
        stride: int = 1,
        kernel_size: int = 3,
        padding: int = 1,
        downsampling: bool = False,
    ):
        super().__init__()

        if out_channels is None:
            out_channels = in_channels

        self.downsampling = downsampling

        self.main_block = nn.Sequential(
            nn.Conv2d(
                in_channels,
                out_channels,
                kernel_size=kernel_size,
                stride=stride,
                padding=padding,
            ),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(
                out_channels,
                out_channels,
                kernel_size=kernel_size,
                # stride=stride,
                padding=padding,
            ),
            nn.BatchNorm2d(out_channels),
        )

        if not downsampling:
            self.res_f = nn.Identity()
        else:
            self.res_f = nn.Sequential(
                nn.Conv2d(
                    in_channels,
                    out_channels,
                    kernel_size=1,
                    stride=stride,
                    # padding=padding,
                ),
                nn.BatchNorm2d(out_channels),
            )

    def forward(self, x):
        residual = x
        out = self.main_block(x)
        residual = self.res_f(residual)
        out += residual
        out = F.relu(out)
        return out
\end{minted}
\label{resnet_torch}
\caption{Ejemplo de implementación con \texttt{pytorch} de un bloque residual.}
\end{figure}

Al implementar esta arquitectura, se muestran resultados experimentales que indican que:
\begin{itemize}
	\item Arquitecturas de redes residuales muy profundas son fácilmente optimizables
	\item El error de sus contrapartes no residuales aumenta conforme aumenta su profundidad
	\item Las redes residuales pueden aumentar su precisión simplemente agregando más bloques
\end{itemize}

\section{Arquitecturas residuales como sistemas dinámicos}
Observemos, brevemente, que cada bloque residual de ResNet puede ser descrito por el siguiente sistema dinamico discreto \cite{E-2017}:

\begin{align}\label{ODE_ResNet}
	    y_l &= h(z_l) + \mathscr{F}(z_l, W_l)\\
	y_{l+1} &= g(y_l)
\end{align}

Donde $z_l$ es la entrada del l-ésima capa, $z_{l+1}$ es la salida de la l-ésima capa, $y_l$ es una variable auxiliar de la l-ésima capa, $h, g$ son mapeos que pueden ser no-lineales. El entrenamiento se determina experimentalemente \cite{he2016identity} que es más facil si sendos $g$ y $h$ son el mapa identidad.

Consideremos $G$ como la inversa de $g$, entonces el sistema dinamico anterior puede ser descrito como

\begin{align}\label{ODE23}
	z_{l+1} = G(h(z_l) + \mathscr{F}(z_l, W_l))
\end{align}

Para que el sistema sea estable (no haya desvanecimiento o explosión de gradientes), el gradiente del lado derecho debe ser cercano a la identidad. Asumiendo $\mathscr{F}$ como una pequeña perturbación, entonces se requiere que $\nabla G \nabla h \approx I$, lo cual se cumple si $g$ y $h$ son mapas identidad. En tal caso, \ref{ODE23} se convierte en:

\begin{align}
	z_{l+1} = z_l + \mathscr{F}(z_l, W_l)
\end{align}

Lo cual se puede ver como una discretización del sistema dinámico

\begin{align}
	\dfrac{dz}{dt} = \mathscr{F}(z, W(t))
\end{align}

Cuya discretización más sencilla toma la forma

\begin{align}
	z_{l+1} = z_l + \Delta t_l \mathscr{F}(z_l, W_l)
\end{align}

Donde $\Delta t_l$ es el tamaño del l-ésimo paso. Notemos que esto es el primer paso del método de Euler.

\subsection{PolyNet}
En \cite{szegedy2014going}, se propone una arquitectura llamada "Inception" cuyos bloques se muestran en \ref{inception_block}.

\begin{figure}[H]
\centering
\includegraphics[height=200px]{images/inception_block.png}
\caption{Bloque de Inception}
\label{inception_block}
\end{figure}

Zhang et al proponen en \cite{zhang2017polynet} utilizar bloques de Inception como bloques residuales. En \ref{polynet_block} se muestra el diagrama original.

\begin{figure}[H]
\centering
\includegraphics[height=200px]{images/polynet.png}
\caption{Bloque de PolyNet}
\label{polynet_block}
\end{figure}

Es necesario aclarar que todos los bloques etiquetados "Inception F" comparten sus parámetros. Configuraciones donde distintos bloques no comparten parámetros son también estudiadas. La motivación principal es estudiar composiciones polinomiales de operadores para explorar la diversidad estructural que permite combinar aditivamente distintos bloques.

Notar que esto resulta en un término de segundo orden respecto de ResNet.

\begin{align}\label{PolyNet_ODE}
	(I + F + F^2) \cdot x &= x + F(x) + F(F(x))
\end{align}

Cuyo lado izquierdo puede reescribirse como
\begin{align}
	I + (I + F) F  = I + F + F^2
\end{align}

De modo tal que su computo sea mas eficiente.

Esta formulación puede verse como un paso del método implicito de Euler

\begin{align}
	u_{n+1} = (I -\Delta tf)^{-1}u_n
\end{align}

Para la ecuación diferencial $u_t = f(u)$. El método implicito permite pasos mas grandes, de modo que son necesarios menos bloques encadenados para lograr clasificación de estado del arte.

\subsection{FractalNet}
FractalNet \cite{larsson2017fractalnet} muestra que el aprendizaje explicito de residuales no es un imperativo para entrenar redes muy profundas.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/fractal_expansion.png}
\caption{Regla de expansión de FractalNet}
\label{fractal_expansion}
\end{figure}

\ref{fractal_expansion} muestra la regla de expansión de FractalNet. Siendo C el indice de un fractal truncado $f_C(\cdot)$, la estructura de FractalNet esta definida por $f_C(\cdot)$. El caso base se define como una red con una sola capa convolucional: $f_1(z) = \text{conv}(z)$.

Los fractales sucesivos se definen como

\begin{align}\label{fractal_net_eq}
	f_{C+1}(z) = \left[ (f_C \circ f_C)(z) \right] \oplus \left[ \text{conv}(z) \right]
\end{align}

Donde $\circ$ denota composicion de funciones y $\oplus$ un operador de unión. Este operador de unión puede ser, por ejemplo, concatenación o suma elemento a elemento. Los autores utilizan el promedio elemento a elemento.

Entonces, \ref{fractal_net_eq} puede ser reescrito como:

\begin{align}
	f_{c+1} = \dfrac{1}{2} f_c \circ f_c + \dfrac{1}{2} k_c
\end{align}

Entonces, cada bloque de FractalNet de orden dos puede ser reescrito como:

\begin{align}
	x_{n+1} = k_1 \times x_n + k_2 \times (k_3 \times x_n + f_1(x_n)) + f_2 (k_3 \times x_n + f_1(x_n))
\end{align}

Lo cual puede ser considerado como un caso de un esquema de Runge-Kutta de segundo orden para la EDO $u_t = f(u,t)$.

\subsection{RevNet}
Recordemos que un sistema dinamico es reversible si su función de transición de estados es biyectiva. Es decir, para cada estado existe un unico estado anterior desde el cual podría haberse llegado.

En \cite{gomez2017reversible} se propone RevNet, una arquitectura compuesta de una serie de bloques reversibles. Los parámetros de cada capa se particionan en dos grupos $x_1, x_2$. Cada bloque reversible toma dos entradas $(x_1, x_2)$ y produce dos salidas $(y_1, y_2)$ acorde a las reglas en \ref{eq_revnet} y funciones residuales $\mathscr{F}$ y $\mathscr{G}$ analogas a ResNet.

\begin{align}\label{eq_revnet}
	y_1 &= x_1 + \mathscr{F}(x_2) \\
	y_2 &= x_2 + \mathscr{G}(y_1)
\end{align}

Por tanto, las activaciones de una capa pueden ser reconstruidas desde las activaciones de la próxima capa:

\begin{align}
	x_2 &= y_2 - \mathscr{G}(y_1) \\
	x_1 &= y_1 - \mathscr{F}(x_2)
\end{align}

De este modo, se ejecuta backpropagation sin guardar las activaciones en memoria.\footnote{La VRAM en GPUs suele ser un limitante en el entrenamiento. Si la arquitectura tiene muchos parámetros, es común utilizar un batch size más chico, lo cual es subóptimo para SGD con mini batches.} De este modo, su costo en memoria es independiente de su profundidad, a excepción de algunas pocas capas no reversibles.

Reformulemos \ref{eq_revnet} como el siguiente sistema dinamico discreto:

\begin{align}
	X_{n+1} &= X_n + \mathscr{F}(Y_n) \\
	Y_{n+1} &= Y_n + \mathscr{G}(X_{n+1})
\end{align}

Con lo cual podemos apreciar su interpretación como un paso del método de Euler del sistema dinámico:

\begin{align}
	\dot{X} &= f_1(Y, t) \\
	\dot{Y} &= f_2(X, t)
\end{align}

\subsection{LM-ResNet}
Notemos que todas las formulaciones anteriores se corresponden con métodos de un solo paso para aproximar sistemas dinámicos. En \cite{lu18d} proponen una arquitectura basada en el método lineal de múltiples pasos.

\subsubsection{Métodos multipasos}
Los métodos de Euler y Runge Kutta mencionados previamente requieren solamente conocer el valor de $y_i$ para poder calcular $y_{i+1}$. En un método multi pasos, se emplean dos o más puntos anteriores para calcular el siguiente punto.

Dado un problema a valores iniciales

\begin{equation}
\left\{ \begin{aligned}
		y\prime &= f(t,y), \enspace a \leq t \leq b\\
		y(a) &= \alpha
\end{aligned} \right.
\end{equation}

Un método multi pasos de orden $m > 1$ es aquel cuya ecuación en diferencias para aproximar $y_{i+1}$ en $t_{i+1}$ se corresponde con la ecuación:

\begin{align}
	y_{i+1} &= a_{m-1} y_i + a_{m-2} y_{i-1} + \ldots + a_0 y_{i+1-m} \\
	        &+ h \left[b_m f(t_{i+1}, y_{i+1}) + b_{m-1} f(t_i, y_i) + \ldots + b_0 f(t_{i+1-m}, y_{i+1-m})  \right]
\end{align}

Con $i=m-1, m, \cdots, N-1$ y donde $a_0, a_1, \cdots, a_{m-1}$ y $b_0, b_1, \cdots, b_m$ son constantes y los valores iniciales $y_0 = \alpha, y_1 = \alpha_1, y_2 = \alpha_2, \cdots, y_{m-1} = \alpha_{m-1}$ son especificados.

Si $b_m = 0$, el método es \textbf{explícito o abierto} ya que el valor de $w_{i+1}$ se da explícitamente en términos de los valores previamente calculados. Cuando $b_m \neq 0$, el método es \textbf{implícito o cerrado}, ya que el valor de $w_{i+1}$ se encuentra en ambos miembros de la ecuación y se especifica sólo implicitamente.

\subsubsection{Formulación de LM-ResNet}
La arquitectura LM puede ser formulada como

\begin{align}\label{LM_arch}
	u_{n+1} = (1-k_n) u_n + k_n u_{n-1} + f_n(u_n)
\end{align}

Donde $k_n \in \mathbb{R}^n$ es un paramétro entrenable de cada capa que actúa como factor de escalado del residuo de la capa actual y la anterior. La figura \ref{lmblock} muestra el bloque de LM-ResNet \textit{acorde al código original}. Notar que \ref{lmblockorig}, el bloque según el paper, presenta diferencias. En primer instancia, el código original multiplica el residuo de la capa previa por $1-k_s$, pero acorde a la ecuación \ref{LM_arch}, y el diagrama \ref{lmblockorig}, se multiplica por $k_s$. El diagrama original es poco claro también, pareciendo indicar que se multiplica $u_n$ tanto por $k_s$ como por $1-k_s$.

\begin{figure}[H]
\centering
\includegraphics[height=200px]{images/LM-Block.png}
\caption{Bloque de LM-ResNet, acorde al código original.}
\label{lmblock}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[height=200px]{images/LM-Block-orig.png}
\caption{Bloque de LM-ResNet, acorde al paper original.}
\label{lmblockorig}
\end{figure}

Para los resultados experimentales, se toma como verdad la ecuación \ref{LM_arch}. De otro modo, deberíamos modificar \ref{LM_arch} como:

\begin{align}
	u_{n+1} = f_n(u_n) + k_n \times u_n + (1-k_n) \times u_{n-1}
\end{align}

\section{Resultados experimentales}
En esta sección se detallan los resultados de distintos experimentos desarrollados a fin de replicar los resultados expuestos en el paper. A estos efectos, se utiliza el código original\footnote{Tomado de \url{https://github.com/2prime/LM-ResNet}} y se arma un pipeline de entrenamiento acorde a lo relatado en el paper.

El código utilizado para obtener estos resultados está disponible en \url{https://github.com/CrossNox/beyond}. Los aportes hechos en este repositorio son:

\begin{itemize}
	\item Corrección del bug antes mencionado
	\item Mejoras varias al código, incluyendo aquellas para reducir el impacto en memoria de entrenamiento
	\item Modularización del código
	\item Armado de notebooks para entrenar modelos
	\item Armado de notebooks para realizar los gráficos resultantes del entrenamiento
\end{itemize}

\subsection{CIFAR10}
Los experimentos aquí detallados se realizan utilizando el dataset CIFAR10. Este dataset se compone de 60000 imágenes de 32x32 píxeles con tres canales de color (RGB). Se divide en 50000 imágenes de entrenamiento y 10000 para el set de pruebas.

Siguiendo lo especificado en \S 2.2, se aplican técnicas de image augmentation al dataset de entrenamiento. En particular, se agrega un padding de 4 píxeles a cada lado, de modo tal que la imagen pasa a tener 40x40 píxeles. Con una probabilidad del 50\% se espeja la imagen horizontalmente. Finalmente, se toma un recorte aleatorio de 32x32 píxeles.

En etapa de pruebas, se toma una vista simple de la imagen original de 32x32. Es importante notar esto dado que el paper de ResNet utiliza otra técnica de evaluación, tomando múltiples vistas de la imagen original.

\subsection{Hiperparámetros relevantes}
A fin de seguir los lineamientos planteados por el paper, se modificó el código original y se implementó un pipeline de entrenamiento con los hiperparámetros especificados. El batch size se configuró como 128 imágenes por batch. Se utiliza SGD como optimizador, con weight decay  configurado como 0.0001 y momentum como 0.9. En \ref{lrcurve} se especifica la curva del parametro de learning rate.

\begin{figure}[H]
\centering
\includegraphics[height=200px]{images/lr_curve.png}
\caption{Curva de learning rate}
\label{lrcurve}
\end{figure}

El entrenamiento se detiene a los 160 epochs.

Todos los experimentos fueron llevados a cabo con CUDA 12.1 en una GPU Nvidia 2070 Super con 8GB de memoria de video, la cual posee un \href{https://developer.nvidia.com/cuda-gpus}{CUDA compute capability} de 7.5.

\subsection{Resultados}

En \ref{tabla1} se resumen los principales resultados de los experimentos realizados. Se indica la precisión obtenida (y su error como contraparte) junto con aquella reportada en el paper. Para que los números reportados sean más correctos, se podría haber tomado el promedio y desviación estándar de varias ejecuciones, pero por motivos de tiempo disponible, no fue posible. Se indica la diferencia entre los errores obtenidos y los reportados en el paper.


% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[H]
\begin{adjustbox}{center}
\begin{tabular}{l|ll|ll|lll|}
\cline{2-8}
 & \multicolumn{2}{c|}{\textbf{Replicación}} & \multicolumn{2}{c|}{\textbf{Paper}} &  &  &  \\ \cline{2-8}
 & \multicolumn{1}{c}{\textbf{Acc.}} & \multicolumn{1}{c|}{\textbf{Error (\%)}} & \multicolumn{1}{c}{\textbf{Acc.}} & \multicolumn{1}{c|}{\textbf{Error (\%)}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Diferencia\\ de error\end{tabular}}} & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Parametros\\ entrenables\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}l@{}}Mult-Add\\ totales (G)\end{tabular}} \\ \hline
\multicolumn{1}{|l|}{\textbf{MResNet-20}} & 0.9200 & 8.00 & 0.9167 & 8.33 & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFC7}+0.33} & \multicolumn{1}{l|}{273475} & 5.22 \\
\multicolumn{1}{|l|}{\textbf{MResNet-32}} & 0.9204 & 7.96 & 0.9282 & 7.18 & \multicolumn{1}{l|}{\cellcolor[HTML]{FF6961}-0.78} & \multicolumn{1}{l|}{467913} & 8.85 \\
\multicolumn{1}{|l|}{\textbf{MResNet-44}} & 0.9279 & 7.21 & 0.9334 & 6.66 & \multicolumn{1}{l|}{\cellcolor[HTML]{FE996B}-0.55} & \multicolumn{1}{l|}{662351} & 12.47 \\
\multicolumn{1}{|l|}{\textbf{MResNet-56}} & 0.9315 & 6.85 & 0.9369 & 6.31 & \multicolumn{1}{l|}{\cellcolor[HTML]{FE996B}-0.54} & \multicolumn{1}{l|}{856789} & 16.10 \\
\multicolumn{1}{|l|}{\textbf{MResNet-110}} & 0.9357 & 6.43 & 0.9384 & 6.16 & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFC7}-0.27} & \multicolumn{1}{l|}{1731760} & 32.40 \\
\multicolumn{1}{|l|}{\textbf{MResNet-164}} & 0.9439 & 5.61 & 0.9473 & 5.27 & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFC7}-0.34} & \multicolumn{1}{l|}{1703760} & 31.70 \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Tabla de resumen de resultados obtenidos.}
\label{tabla1}
\end{table}

Un detalle que resulta confuso es que la cantidad de parámetros entrenables de ResNet110 no se condice con lo reportado en el paper (en este experimento dió 1.73M, vs 1.14M en el paper). La cantidad de Mult-Add da una idea del costo computacional de inferencia de cada modelo.

Los errores reportados dan \textit{razonablemente} dentro de lo esperado, sin notar diferencias que sean irreconciliables con lo reportado en el paper. A finales de 2017, fecha de publicación del paper bajo estudio, el SOTA para CIFAR10 era\footnote{\url{https://paperswithcode.com/sota/image-classification-on-cifar-10}} de 97.88\% de precisión \cite{hu2019squeezeandexcitation}, por lo que la diferencia en ese momento aún era de alrededor de 3.5\% de precisión.

En \ref{test_set_acc} se muestra la curva de precisión sobre el set de test para cada uno de los modelos entrenados. Notemos que al cambiar el learning rate, los valores se estabilizan y se observa lo que los autores proclaman: agregar más capas en efecto aumenta la precisión sin notarse cambios grandes de comportamiento en \ref{train_loss}. En dicho gráfico se ven las curvas de pérdida para cada modelo durante entrenamiento, tanto para el set de pruebas como de entrenamiento. En \ref{train_loss_log} se ven las curvas en escala logarítmica para apreciar mejor la diferencia entre ambas curvas. Si bien se puede observar que la curva de pérdida de prueba aumenta luego del cambio de learning rate, su precisión mejora.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/accuracy.png}
\caption{Accuracy sobre el set de test}
\label{test_set_acc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/loss.png}
\caption{Pérdida (cross-entropy) de cada modelo durante entrenamiento}
\label{train_loss}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/loss_log.png}
\caption{Pérdida (cross-entropy) de cada modelo durante entrenamiento. Escala logarítmica.}
\label{train_loss_log}
\end{figure}

En \ref{momentum} y \ref{momentum_large} se ven los valores aprendidos por los parámetros $k_n$, los momentos de cada una de las capas residuales. Este punto es donde mayor discrepancias se encuentran respecto del paper original. En primer instancia, el paper proclama que estos valores se inicializan $\sim \mathscr{U}(-0.1, 0)$, sin embargo en el código original~\footnote{\url{https://github.com/2prime/LM-ResNet/blob/ff49654a50abd76f9fd40342a6d7b82e3ff11c31/MResNet.py\#L139}} los valores se inicializan en $\mathscr{U}(1.0, 1.1)$. Tal como se hizo antes, se toma lo explicado en el paper como verdad, y se modifica el código acorde. Notemos que esto, sin embargo, hace que ambas discrepancias tomen sentido: $1 - \mathscr{U}(1.0, 1.1) \sim \mathscr{U}(-0.1, 0.0)$. Sin embargo, por claridad, se prefiere dejar el código corregido para cumplir con lo descrito explícitamente. En segunda instancia, los momentos reportados en el paper decaen conforme se avanza por la red y se distribuyen en $\left[-2.0, 1.0\right]$. En los experimentos de replicación, vemos que los valores se mantienen en $\left[-0.1, 0.0\right]$. Aún luego de varias revisiones, no se evidencia un error en el código por el cual estos valores no se modifiquen con el tiempo.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/momentum.png}
\caption{Momento aprendido en cada capa.}
\label{momentum}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/momentum_large.png}
\caption{Momento aprendido en cada capa (modelos grandes).}
\label{momentum_large}
\end{figure}

\section{Sistemas dinámicos estocásticos}
En \cite{huang2016deep}, se propone, aleatoriamente eliminar un subset de capas y reemplazarlas por la función identidad. Se inicia con una red extremadamente profunda (incluso más de 1200 capas) para luego ir eliminando algunas durante cada epoch de entrenamiento. Por otro lado, \cite{gastaldi2017shakeshake} propone utilizar regularización shake-shake para mejorar los resultados de una red con múltiples ramas (como LM-Resnet), reemplazando la operación de suma como unión de ramas por una operación afín estocástica.

\subsection{Sistema dinámico de una regularización Shake-Shake}
Tal como se adelantó, en \cite{gastaldi2017shakeshake} se propone la siguiente combinación afin estocástica:

\begin{align}
	X_{n+1} = X_n + \eta f_1(X_n) + (1-\eta) f_2 (X_n)
\end{align}

Como operador de unión de múltiples componentes de un bloque residual. $\eta \sim \mathscr{U}(0,1)$. Introduciendo un paso temporal $\Delta t$ encontramos su sistema dinamico estocástico asociado:

\begin{align}\label{shakeshake_step}
	X_{n+1} = X_n + \left(\dfrac{\Delta t}{2} + \sqrt{\Delta t}(\eta - \dfrac{1}{2})\right) f_1(X_n) + \left( \dfrac{\Delta t}{2} + \sqrt{\Delta t} (\dfrac{1}{2} - \eta) \right) f_2(X_n)
\end{align}

Cuando $\Delta t \equiv 1$, la ecuación anterior se reduce a la regularización shake-shake, pero cuando se elige $\Delta t \neq 1$, entonces se tienen alternativas a la misma. En el paper se deriva, desde \ref{shakeshake_step}, que la red correspondiente a la regularización shake-shake es una aproximación débil del siguiente sistema dinámico estocástico:

\begin{align}
	dX &= \dfrac{1}{2} (f_1(X) + f_2(X)) dt\\
	   &+ \dfrac{1}{\sqrt{12}} (f_1(X) - f_2(X)) \odot \left[ \mathbbm{1}_{N\times 1}, 0_{N,N-1} \right] dB_t
\end{align}

Donde $dB_t$ es un vector de $N$ dimensiones de movimiento Browniano.

\subsection{Profundidad estocástica}
En \cite{huang2016deep}, como se adelantó, se propone eliminar bloques residuales aleatoriamente durante entrenamiento. Esto obedece a varios objetivos: en primer lugar, reducir el tiempo de entrenamiento. En segundo lugar, mejorar la robustez de la red. Es decir, actúa como método de regularización.

El paso hacia adelante de propagación de gradientes se vería como:

\begin{align}
	X_{n+1} = X_n + \eta_n f(X_n)
\end{align}

Donde $\mathbb{P}(\eta_n = 1) = p_n, \mathbb{P}(\eta_n = 0) = 1 - p_n$. Al introducir un paso temporal $\Delta t \equiv 1$, se deriva que la red con dropout estocástico se reduce a una aproximación débil del sistema estocástico dinámico

\begin{align}
	dX = p(t) f(X) dt + \sqrt{p(t) (1-p(t))} f(X) \odot [\mathbbm{1}_{N \times 1}, 0_{N, N - 1}] dB_t
\end{align}

\subsection{Entrenamiento estocástico para LM-ResNet}
En el paper se deriva la siguiente estrategia estocástica de entrenamiento para la arquitectura LM:

\begin{align}
	X_{n+1} = (2 + g_n) X_n - (1 + g_n) x_{n-1} + \eta_n f(X_n)
\end{align}

Donde $\mathbb{P}(\eta_n = 1) = p_n, \mathbb{P}(\eta_n = 0) = 1 - p_n$. Lo cual indicaría que se puede implementar simplemente eliminando el bloque residual con probabilidad $p$.

\subsection{Resultados experimentales}
\subsubsection{Hiperparámetros}
El dataset se trata del mismo modo que para experimentos anteriores. En este caso, la probabilidad de eliminar un bloque residual en cada capa es un funcion lineal de la capa. Siendo $l$ la capa bajo consideración, la probabilidad de eliminar un bloque residual es $\dfrac{l}{L}(1 - p_L)$, donde $L$ refiere a la profundidad de la red y $p_L$ es la probabilidad asociada a la capa anterior a $l$.

Para LM-ResNet56 se selecciona $p_L = 0.8$ y para LM-ResNet110 se selecciona $p_L = 0.5$. Se entrena utilizando descenso estocástico por gradiente con decaimiento de peso de 0.0001 y momento de 0.9. El factor de aprendizaje arranca en 0.1, pasa a ser 0.01 luego del epoch 250 y 0.001 luego del epoch 375. El entrenamiento termina a los 500 epochs.

\subsubsection{Resultados obtenidos}

En \ref{tabla2} se resumen los principales resultados de los experimentos de entrenamiento estocástico. Estos experimentos, considerando que tienen un costo computacional más alto y entrenan por 3 veces más epochs que las anteriores pruebas, tardan mas que las pruebas para los 6 modelos sin entrenamiento estocástico. Se utiliza regularización dropout acorde a las pruebas reportadas en el paper. Se implementa la regularización shake-shake mediante la suma de ruido uniforme a la salida de cada bloque residual. Sin embargo, al no tener un valor de referencia en el paper, no se ejecutaron las pruebas.

Vemos que los valores de precisión obtenidos se corresponden con los reportados, dentro de márgenes de error aceptables para ambas configuraciones de la arquitectura LM-ResNet. Resulta destacable que LM-ResNet56 con entrenamiento estocástico tiene un error menor que LM-ResNet110 y LM-ResNet164 sin entrenamiento estocástico, y con la mitad de parámetros. LM-ResNet110 con entrenamiento estocástico logra una mejora de 6.8\% error relativo respecto de LM-ResNet164 sin entrenamiento estocástico y \textbf{20\%} respecto de LM-ResNet110 sin entrenamiento estocástico.

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[H]
\begin{adjustbox}{center}
\begin{tabular}{l|ll|ll|lll|}
\cline{2-8}
 & \multicolumn{2}{c|}{\textbf{Replicación}} & \multicolumn{2}{c|}{\textbf{Paper}} &  &  &  \\ \cline{2-8}
 & \multicolumn{1}{c}{\textbf{Acc.}} & \multicolumn{1}{c|}{\textbf{Error (\%)}} & \multicolumn{1}{c}{\textbf{Acc.}} & \multicolumn{1}{c|}{\textbf{Error}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Diferencia\\ de error\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Parámetros\\ entrenables\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Mult-Add\\ totales (G)\end{tabular}}} \\ \hline
\multicolumn{1}{|l|}{\textbf{MResNet-56}} & 0.9484 & 5.16 & 0.9486 & 5.14 & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFC7}-0.02} & \multicolumn{1}{l|}{856789} & 16.10 \\
\multicolumn{1}{|l|}{\textbf{MResNet-110}} & 0.9509 & 4.91 & 0.9520 & 4.80 & \multicolumn{1}{l|}{\cellcolor[HTML]{FFCE93}-0.11} & \multicolumn{1}{l|}{1731760} & 32.40 \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Tabla de resumen de resultados obtenidos para entrenamiento estocástico.}
\label{tabla2}
\end{table}

En \ref{sd_test_set_acc} se muestra la curva de precisión sobre el set de test para ambos modelos entrenados con entrenamiento estocástico. Se observa un comportamiento similar al de experimentos anteriores donde al disminuir el learning rate se estabiliza la precisión. En \ref{train_loss_sd} se observa la pérdida durante entrenamiento tanto para el set de test como de entrenamiento. Las diferencias enter ambas curvas se observan mejor en \ref{train_loss_log_sd}. En éste último gráfico se observa un fenómeno no observado previamente: la pérdida de entrenamiento comienza a subir antes de volver a disminuir el learning rate a 0.001, donde vuelve a bajar.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/accuracy_sd.png}
\caption{Accuracy sobre el set de test}
\label{sd_test_set_acc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/loss_sd.png}
\caption{Pérdida (cross-entropy) de cada modelo durante entrenamiento}
\label{train_loss_sd}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/loss_log_sd.png}
\caption{Pérdida (cross-entropy) de cada modelo durante entrenamiento. Escala logarítmica.}
\label{train_loss_log_sd}
\end{figure}

Tal como en experimentos anteriores, en \ref{sd_momentum} se observa que el comportamiento de los valores $k_n$ aprendidos para cada capa residual son aleatorios dentro del rango de inicialización.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/momentum_sd.png}
\caption{Momento aprendido en cada capa .}
\label{sd_momentum}
\end{figure}

\section{Una nota sobre el método de ecuaciones modificadas}
La ecuación modificada de un esquema de análisis numérico que aproxima un sistema de ecuaciones por diferencias es otra ecuación diferencial que puede ser mejor aproximado por el esquema numérico. Se utilizan para describir las propiedades de dichos esquemas. La conclusión relevante del paper es que cuando $f$, la función aproximada por el esquema de Euler de primer orden de ResNet, es un flojo de gradientes, la ecuación de diferencias de LM-ResNet tiene una condición de estabilidad $-1 \leq k_n \leq 1$.

\section{Conclusión}
En el presente trabajo se explica el paper ``Beyond Finite Layer Neural Networks'', haciendo un repaso del estado del arte previo, su motivación y los temas más relevantes de la literatura relacionada. Se implementan los pipelines de entrenamientos necesarios para replicar los resultados, se reporta la diferencia obtenida respecto de los resultados mencionados en el paper y se hacen notar varios problemas en el paper original y el código.

Una observación que destaca es que los momentos $k_n$ no parecen tener relevancia en el resultado final de la red. Si bien se replican los resultados, no parece que aprender estos valores de escala de cada paso de un método numérico de dos pasos resulte condición necesaria para el resultado final.

Considero que los objetivos del trabajo han sido cumplidos, sin embargo, se abren las puertas a varias líneas de mejora sucesivas.

\subsection{Trabajo futuro propuesto}
Un camino que no see exploró es el de replicar los resultados para CIFAR100 e ImageNet. Esto se debió a los tiempos de entrenamiento necesarios, pero principalmente debido a que no parecía aportar demasiado respecto de las pruebas realizadas. Se propone también hacer una revisión sobre los valores de $k_n$ para ver si estos están siendo considerados durante la backpropagation de los gradientes.

En \href{https://paperswithcode.com/sota/image-classification-on-cifar-10}{la página de SOTA de CIFAR10 de Papers with Code} se ve que desde la publicación del paper objeto de estudio del presente informe, el estado del arte movió el mejor valor reportado de 97.02\% a 99.5\% en 2021. Desde entonces, y hasta la fecha en 2023, no se han reportado mejores resultados.

Hay dos hechos notables que pienso que ofrecen una oportunidad interesante de investigación. Por un lado, \cite{kolesnikov2020big} introduce BiT, una arquitectura basada en ResNet-v2 que marca un nuevo récord de precisión sobre CIFAR10. Este récord se mantiene desde 2020 hasta que \cite{dosovitskiy2021image} introduce ViT-H/14 en 2021. Esta nueva arquitectura está basada en \cite{vaswani2023attention} la arquitectura Transformer, en la cual se basan las arquitecturas de mejor performance a la fecha, en diferentes dominios como NLP y Computer Vision.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/TransformerBlock.png}
\caption{Bloque de transformer}
\label{transformer}
\end{figure}

En primer lugar, propongo adaptar la arquitectura LM a BiT y evaluar si los resultados obtenidos mejoran respecto de la publicación original. Por otro lado, si observamos en \ref{transformer}, vemos que tanto el encoder como el decoder de la arquitectura de Transformer poseen conexiones residuales, lo cual lleva a pensar que quizás se puedan aplicar los mismos principios aquí estudiados. Sin embargo, resulta más interesante que \cite{lu2019understanding}, con primer autor al mismo autor del paper objecto de estudio del presente informe, estudia a la arquitectura de Transformers como un método de resolucion de EDOs para una ecuación se convección-difusión en un sistema dinámico de múltiples partículas. Propongo evaluar agregar este paper a la lista de papers posibles sobre los cuales realizar una monografía dentro del marco de la materia.

\newpage

\printbibliography

\end{document}
